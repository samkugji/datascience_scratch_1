{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 주피터노트북 cell 너비 조정\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영화 댓글을 이용한 감성 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bitsadmin /transfer get https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt %cd%\\ratings_train.txt\n",
    "# !bitsadmin /transfer get https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt %cd%\\ratings_test.txt    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  mac / linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
    "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "with codecs.open(\"ratings_train.txt\", encoding='utf-8') as f:\n",
    "    train = [line.split('\\t') for line in f.read().splitlines()]\n",
    "    train = train[1:]   # header 제외\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"ratings_test.txt\", encoding='utf-8') as f:\n",
    "    test = [line.split('\\t') for line in f.read().splitlines()]\n",
    "    test = test[1:]   # header 제외\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나이브베이즈 모델을 이용한 감성 분석 (Sentiment Analysis)\n",
    "- 네이버 영화 리뷰\n",
    "    - 평점 3점 이상이면 긍정 / 3점 미만이면 부정\n",
    "- 사전확률 계산\n",
    "- likelihood 계산\n",
    "- 모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['9976970', '아 더빙.. 진짜 짜증나네요 목소리', '0'],\n",
       " ['3819312', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '1'],\n",
       " ['10265843', '너무재밓었다그래서보는것을추천한다', '0']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['6270596', '굳 ㅋ', '1'],\n",
       " ['9274899', 'GDNTOPCLASSINTHECLUB', '0'],\n",
       " ['8544678', '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아', '0']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 사전확률 계산\n",
    "- ## $ P(y = C_k) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_target(training_set):\n",
    "    from collections import defaultdict, Counter\n",
    "    counts = defaultdict(int)\n",
    "    for i, row in enumerate(training_set):\n",
    "        sentiment = row[2]\n",
    "        if sentiment == '1':\n",
    "            counts[sentiment] += 1\n",
    "        else:\n",
    "            counts[sentiment] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'0': 75173, '1': 74827})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_target(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_target(training_set):\n",
    "    pos_prob = count_target(test)['1']/sum(count_target(test).values())\n",
    "    neg_prob = count_target(test)['0']/sum(count_target(test).values())\n",
    "    return pos_prob, neg_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.50346, 0.49654)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_target(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. likelihood 계산\n",
    "- ## $P(x|y=C_k)$\n",
    "- Laplace Smoothing\n",
    "    - ### $\\frac {N_i + \\alpha}{N+\\alpha K}$\n",
    "    - 매우 작은 값을 추가하여 값이 0이 되지 않도록 한다. \n",
    "    - 여러값의 곱을 취할 경우 하나만 0이되면 전체가 0이 되는 문제가 있기 때문이다.\n",
    "        - ex. '메가박스'라는 단어가 긍정을 표현한 리뷰에만 포함된 경우\n",
    "            - P(메가박스 | C_neg) = 0 이 되므로 P(C_neg | 메가박스) 도 0이 되어버린다.\n",
    "            - 라플라스 스무딩 적용 시, P(메가박스 | C_neg) = k / n+k 가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 단어별 카운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(training_set):\n",
    "    from collections import defaultdict\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for i, row in enumerate(training_set):\n",
    "        review_words = row[1].split()\n",
    "        sentiment = row[2]\n",
    "        for word in review_words:\n",
    "            counts[word][0 if sentiment == '1' else 1] += 1 ## word count | C_pos, word count | C_neg\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'정우성': [2, 3]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'정우성' : [2, 3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 단어별 확률계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probabilities(counts, target, k=1):\n",
    "    \"\"\"laplace smoothing 적용\"\"\"\n",
    "    from collections import defaultdict\n",
    "    probabilities = defaultdict(dict)\n",
    "    total_pos = target['1']\n",
    "    total_neg = target['0']\n",
    "    for w, (positive, negative) in counts.items():\n",
    "        probabilities[w] = ((positive + k) / (total_pos + k),  ## P(word | C_pos)\n",
    "                            (negative + k) / (total_neg + k))  ## P(word | C_neg)\n",
    "        \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0002271876837547442, 0.00017293213078990076)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_probabilities(count_words(train), count_target(train))['정우성']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 사후확률 계산\n",
    "- ## $P(y=C_k|x)$\n",
    "- 언더플로우 방지를 위한 log 연산\n",
    "    - 확률 계산 시 확률을 계속 곱해주게되면 매우 낮은 값들니 나와 언더플로우 현상이 발생할 수 있다.\n",
    "    - 이를 방지하기 위해 $P(x∣y=C_{ k })$를 $log(P(x∣y=C_{ k }))$ 로 바꿔서 연산한다. \n",
    "    - ## $log(P(x∣y=C_{ k })) =\\sum _{i=1}^{P}log({P(x_j|y=C_k)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_probability(word_probs, prob_target, review):\n",
    "    import math\n",
    "    review_words = review.split()\n",
    "    log_prob_if_neg = log_prob_if_pos = 0.0\n",
    "    pos_prob = prob_target[0]\n",
    "    neg_prob = prob_target[1]\n",
    "    \n",
    "    for word in review_words:\n",
    "        # 긍정 확률\n",
    "        if word in word_probs:\n",
    "            log_prob_if_pos += math.log(word_probs[word][0])\n",
    "            log_prob_if_neg += math.log(word_probs[word][1])\n",
    "        else:\n",
    "            pass\n",
    "#             log_prob_if_pos += math.log((0 + 1)/self.total_pos + 1)\n",
    "#             log_prob_if_neg += math.log((0 + 1)/self.total_neg + 1)\n",
    "    prob_if_pos = log_prob_if_pos + math.log(pos_prob) ## log(P(x|C_pos)P(C_pos))\n",
    "    prob_if_neg = log_prob_if_neg + math.log(neg_prob) ## log(P(x|C_neg)P(C_neg))\n",
    "    return prob_if_pos , prob_if_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probs = word_probabilities(count_words(train), count_target(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prob = prob_target(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8544678', '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아', '0']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 리뷰 확률 :  -62.30355173446977\n",
      "부정 리뷰 확률 :  -59.931758953659916\n"
     ]
    }
   ],
   "source": [
    "pos_proba, neg_proba = sentiment_probability(word_probs, target_prob, test[2][1])\n",
    "print('긍정 리뷰 확률 : ', pos_proba)\n",
    "print('부정 리뷰 확률 : ', neg_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k = 1):\n",
    "        self.k = k\n",
    "        \n",
    "    def count_target(self, training_set):\n",
    "        from collections import defaultdict\n",
    "        counts = defaultdict(int)\n",
    "        for i, row in enumerate(training_set):\n",
    "            sentiment = row[2]\n",
    "            if sentiment == '1':\n",
    "                counts[sentiment] += 1\n",
    "            else:\n",
    "                counts[sentiment] += 1\n",
    "        return counts\n",
    "    \n",
    "    def prob_target(self, training_set):\n",
    "        pos_prob = self.count_target(training_set)['1']/sum(self.count_target(training_set).values())\n",
    "        neg_prob = self.count_target(training_set)['0']/sum(self.count_target(training_set).values())\n",
    "        return pos_prob, neg_prob\n",
    "\n",
    "    def count_words(self, training_set):\n",
    "        from collections import defaultdict\n",
    "        counts = defaultdict(lambda: [0, 0])\n",
    "        for i, row in enumerate(training_set):\n",
    "            review_words = row[1].split()\n",
    "            sentiment = row[2]\n",
    "            for word in review_words:\n",
    "                counts[word][0 if sentiment == '1' else 1] += 1      \n",
    "        return counts\n",
    "\n",
    "    def word_probabilities(self, counts, target, k=1):\n",
    "        from collections import defaultdict\n",
    "        \"\"\"laplace smoothing 적용\"\"\"\n",
    "        probabilities = defaultdict(dict)\n",
    "        self.total_pos = target['1']\n",
    "        self.total_neg = target['0']\n",
    "        for w, (positive, negative) in counts.items():\n",
    "            probabilities[w] = ((positive + k) / (self.total_pos + k), \n",
    "                                (negative + k) / (self.total_neg + k))\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    def sentiment_probability(self, word_prob, review):\n",
    "        import math\n",
    "        review_words = review.split()\n",
    "        log_prob_if_neg = log_prob_if_pos = 0.0\n",
    "        pos_prob = self.pos_prob\n",
    "        neg_prob = self.neg_prob\n",
    "        \n",
    "        for word in review_words:\n",
    "            # 긍정 확률\n",
    "            if word in word_prob:\n",
    "                log_prob_if_pos += math.log(word_prob[word][0])\n",
    "                log_prob_if_neg += math.log(word_prob[word][1])\n",
    "            else:\n",
    "                log_prob_if_pos += math.log((0 + self.k)/self.total_pos + self.k)\n",
    "                log_prob_if_neg += math.log((0 + self.k)/self.total_neg + self.k)\n",
    "                \n",
    "        prob_if_pos = log_prob_if_pos + math.log(pos_prob)\n",
    "        prob_if_neg = log_prob_if_neg + math.log(neg_prob)\n",
    "        return prob_if_pos , prob_if_neg\n",
    "    \n",
    "    def train(self, training_set):\n",
    "        self.pos_prob, self.neg_prob = self.prob_target(training_set)\n",
    "        word_counts = self.count_words(training_set)\n",
    "        target_counts = self.count_target(training_set)\n",
    "        self.word_prob = self.word_probabilities(word_counts, target_counts, k = self.k)\n",
    "    \n",
    "    def predict(self, review):\n",
    "        prob_if_pos , prob_if_neg = self.sentiment_probability(self.word_prob, review)\n",
    "        if prob_if_pos > prob_if_neg:\n",
    "            return '1'\n",
    "        else:\n",
    "            return '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_target = []\n",
    "predicted_target = []\n",
    "for i, row in enumerate(test):\n",
    "    true_target.append(row[2])\n",
    "    predicted_target.append(classifier.predict(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.83      0.81     24827\n",
      "          1       0.82      0.78      0.80     25173\n",
      "\n",
      "avg / total       0.81      0.80      0.80     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_target, predicted_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(zip(*train))[1]\n",
    "y = np.array(list(zip(*train))[2], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = list(zip(*test))[1]\n",
    "y_test = np.array(list(zip(*test))[2], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "## http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "model = Pipeline([\n",
    "            ('vect', CountVectorizer()), \n",
    "            ('mb', MultinomialNB()),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('mb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.84      0.83     24827\n",
      "          1       0.84      0.81      0.82     25173\n",
      "\n",
      "avg / total       0.83      0.83      0.83     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline 없이 사용\n",
    "\n",
    "vectorizer = CountVectorizer().fit(X)\n",
    "x_vec = vectorizer.transform(X)\n",
    "model = MultinomialNB().fit(x_vec, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.84      0.83     24827\n",
      "          1       0.84      0.81      0.82     25173\n",
      "\n",
      "avg / total       0.83      0.83      0.83     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(vectorizer.transform(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트에서 노이즈 제거 후 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "t = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adjective': '형용사',\n",
       " 'Adverb': '부사',\n",
       " 'Alpha': '알파벳',\n",
       " 'Conjunction': '접속사',\n",
       " 'Determiner': '관형사',\n",
       " 'Eomi': '어미',\n",
       " 'Exclamation': '감탄사',\n",
       " 'Foreign': '외국어, 한자 및 기타기호',\n",
       " 'Hashtag': '트위터 해쉬태그',\n",
       " 'Josa': '조사',\n",
       " 'KoreanParticle': '(ex: ㅋㅋ)',\n",
       " 'Noun': '명사',\n",
       " 'Number': '숫자',\n",
       " 'PreEomi': '선어말어미',\n",
       " 'Punctuation': '구두점',\n",
       " 'ScreenName': '트위터 아이디',\n",
       " 'Suffix': '접미사',\n",
       " 'Unknown': '미등록어',\n",
       " 'Verb': '동사'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tagset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 전처리\n",
    "    - 조사 등 의미없는 형태소는 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_set = ['Adjective', 'Adverb', 'Noun', 'Verb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000it [05:20, 467.42it/s]\n",
      "50000it [02:25, 344.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "train_proc = copy.deepcopy(train)\n",
    "test_proc = copy.deepcopy(test)\n",
    "\n",
    "for _, row in tqdm(enumerate(train_proc)):\n",
    "    row[1] = ' '.join([pos[0] for pos in t.pos(row[1]) if pos[1] in tag_set])\n",
    "    \n",
    "for _, row in tqdm(enumerate(test_proc)):\n",
    "    row[1] = ' '.join([pos[0] for pos in t.pos(row[1]) if pos[1] in tag_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3819312', '흠 포스터 보고 초딩 영화 줄 오버 연기 가볍 않구', '1']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_proc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3819312', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '1']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_1 = NaiveBayesClassifier()\n",
    "classifier_1.train(train_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_target = []\n",
    "predicted_target = []\n",
    "for i, row in enumerate(test_proc):\n",
    "    true_target.append(row[2])\n",
    "    predicted_target.append(classifier_1.predict(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.87      0.84     24827\n",
      "          1       0.86      0.80      0.83     25173\n",
      "\n",
      "avg / total       0.84      0.84      0.84     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_target, predicted_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 데이터 분석시, 전처리(노이즈 제거)가 매우 중요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
